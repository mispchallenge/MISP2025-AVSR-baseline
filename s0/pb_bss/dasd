Help on package sklearn.mixture in sklearn:

NNAAMMEE
    sklearn.mixture - The :mod:`sklearn.mixture` module implements mixture modeling algorithms.

PPAACCKKAAGGEE  CCOONNTTEENNTTSS
    _base
    _bayesian_mixture
    _gaussian_mixture
    tests (package)

CCLLAASSSSEESS
    sklearn.mixture._base.BaseMixture(sklearn.base.DensityMixin, sklearn.base.BaseEstimator)
        sklearn.mixture._bayesian_mixture.BayesianGaussianMixture
        sklearn.mixture._gaussian_mixture.GaussianMixture
    
    class BBaayyeessiiaannGGaauussssiiaannMMiixxttuurree(sklearn.mixture._base.BaseMixture)
     |  BayesianGaussianMixture(*, n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=None, mean_precision_prior=None, mean_prior=None, degrees_of_freedom_prior=None, covariance_prior=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)
     |  
     |  Variational Bayesian estimation of a Gaussian mixture.
     |  
     |  This class allows to infer an approximate posterior distribution over the
     |  parameters of a Gaussian mixture distribution. The effective number of
     |  components can be inferred from the data.
     |  
     |  This class implements two types of prior for the weights distribution: a
     |  finite mixture model with Dirichlet distribution and an infinite mixture
     |  model with the Dirichlet Process. In practice Dirichlet Process inference
     |  algorithm is approximated and uses a truncated distribution with a fixed
     |  maximum number of components (called the Stick-breaking representation).
     |  The number of components actually used almost always depends on the data.
     |  
     |  .. versionadded:: 0.18
     |  
     |  Read more in the :ref:`User Guide <bgmm>`.
     |  
     |  Parameters
     |  ----------
     |  n_components : int, default=1
     |      The number of mixture components. Depending on the data and the value
     |      of the `weight_concentration_prior` the model can decide to not use
     |      all the components by setting some component `weights_` to values very
     |      close to zero. The number of effective components is therefore smaller
     |      than n_components.
     |  
     |  covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'
     |      String describing the type of covariance parameters to use.
     |      Must be one of::
     |  
     |          'full' (each component has its own general covariance matrix),
     |          'tied' (all components share the same general covariance matrix),
     |          'diag' (each component has its own diagonal covariance matrix),
     |          'spherical' (each component has its own single variance).
     |  
     |  tol : float, default=1e-3
     |      The convergence threshold. EM iterations will stop when the
     |      lower bound average gain on the likelihood (of the training data with
     |      respect to the model) is below this threshold.
     |  
     |  reg_covar : float, default=1e-6
     |      Non-negative regularization added to the diagonal of covariance.
     |      Allows to assure that the covariance matrices are all positive.
     |  
     |  max_iter : int, default=100
     |      The number of EM iterations to perform.
     |  
     |  n_init : int, default=1
     |      The number of initializations to perform. The result with the highest
     |      lower bound value on the likelihood is kept.
     |  
     |  init_params : {'kmeans', 'random'}, default='kmeans'
     |      The method used to initialize the weights, the means and the
     |      covariances.
     |      Must be one of::
     |  
     |          'kmeans' : responsibilities are initialized using kmeans.
     |          'random' : responsibilities are initialized randomly.
     |  
     |  weight_concentration_prior_type : str, default='dirichlet_process'
     |      String describing the type of the weight concentration prior.
     |      Must be one of::
     |  
     |          'dirichlet_process' (using the Stick-breaking representation),
     |          'dirichlet_distribution' (can favor more uniform weights).
     |  
     |  weight_concentration_prior : float | None, default=None.
     |      The dirichlet concentration of each component on the weight
     |      distribution (Dirichlet). This is commonly called gamma in the
     |      literature. The higher concentration puts more mass in
     |      the center and will lead to more components being active, while a lower
     |      concentration parameter will lead to more mass at the edge of the
     |      mixture weights simplex. The value of the parameter must be greater
     |      than 0. If it is None, it's set to ``1. / n_components``.
     |  
     |  mean_precision_prior : float | None, default=None.
     |      The precision prior on the mean distribution (Gaussian).
     |      Controls the extent of where means can be placed. Larger
     |      values concentrate the cluster means around `mean_prior`.
     |      The value of the parameter must be greater than 0.
     |      If it is None, it is set to 1.
     |  
     |  mean_prior : array-like, shape (n_features,), default=None.
     |      The prior on the mean distribution (Gaussian).
     |      If it is None, it is set to the mean of X.
     |  
     |  degrees_of_freedom_prior : float | None, default=None.
     |      The prior of the number of degrees of freedom on the covariance
     |      distributions (Wishart). If it is None, it's set to `n_features`.
     |  
     |  covariance_prior : float or array-like, default=None.
     |      The prior on the covariance distribution (Wishart).
     |      If it is None, the emiprical covariance prior is initialized using the
     |      covariance of X. The shape depends on `covariance_type`::
     |  
     |              (n_features, n_features) if 'full',
     |              (n_features, n_features) if 'tied',
     |              (n_features)             if 'diag',
     |              float                    if 'spherical'
     |  
     |  random_state : int, RandomState instance or None, default=None
     |      Controls the random seed given to the method chosen to initialize the
     |      parameters (see `init_params`).
     |      In addition, it controls the generation of random samples from the
     |      fitted distribution (see the method `sample`).
     |      Pass an int for reproducible output across multiple function calls.
     |      See :term:`Glossary <random_state>`.
     |  
     |  warm_start : bool, default=False
     |      If 'warm_start' is True, the solution of the last fitting is used as
     |      initialization for the next call of fit(). This can speed up
     |      convergence when fit is called several times on similar problems.
     |      See :term:`the Glossary <warm_start>`.
     |  
     |  verbose : int, default=0
     |      Enable verbose output. If 1 then it prints the current
     |      initialization and each iteration step. If greater than 1 then
     |      it prints also the log probability and the time needed
     |      for each step.
     |  
     |  verbose_interval : int, default=10
     |      Number of iteration done before the next print.
     |  
     |  Attributes
     |  ----------
     |  weights_ : array-like of shape (n_components,)
     |      The weights of each mixture components.
     |  
     |  means_ : array-like of shape (n_components, n_features)
     |      The mean of each mixture component.
     |  
     |  covariances_ : array-like
     |      The covariance of each mixture component.
     |      The shape depends on `covariance_type`::
     |  
     |          (n_components,)                        if 'spherical',
     |          (n_features, n_features)               if 'tied',
     |          (n_components, n_features)             if 'diag',
     |          (n_components, n_features, n_features) if 'full'
     |  
     |  precisions_ : array-like
     |      The precision matrices for each component in the mixture. A precision
     |      matrix is the inverse of a covariance matrix. A covariance matrix is
     |      symmetric positive definite so the mixture of Gaussian can be
     |      equivalently parameterized by the precision matrices. Storing the
     |      precision matrices instead of the covariance matrices makes it more
     |      efficient to compute the log-likelihood of new samples at test time.
     |      The shape depends on ``covariance_type``::
     |  
     |          (n_components,)                        if 'spherical',
     |          (n_features, n_features)               if 'tied',
     |          (n_components, n_features)             if 'diag',
     |          (n_components, n_features, n_features) if 'full'
     |  
     |  precisions_cholesky_ : array-like
     |      The cholesky decomposition of the precision matrices of each mixture
     |      component. A precision matrix is the inverse of a covariance matrix.
     |      A covariance matrix is symmetric positive definite so the mixture of
     |      Gaussian can be equivalently parameterized by the precision matrices.
     |      Storing the precision matrices instead of the covariance matrices makes
     |      it more efficient to compute the log-likelihood of new samples at test
     |      time. The shape depends on ``covariance_type``::
     |  
     |          (n_components,)                        if 'spherical',
     |          (n_features, n_features)               if 'tied',
     |          (n_components, n_features)             if 'diag',
     |          (n_components, n_features, n_features) if 'full'
     |  
     |  converged_ : bool
     |      True when convergence was reached in fit(), False otherwise.
     |  
     |  n_iter_ : int
     |      Number of step used by the best fit of inference to reach the
     |      convergence.
     |  
     |  lower_bound_ : float
     |      Lower bound value on the likelihood (of the training data with
     |      respect to the model) of the best fit of inference.
     |  
     |  weight_concentration_prior_ : tuple or float
     |      The dirichlet concentration of each component on the weight
     |      distribution (Dirichlet). The type depends on
     |      ``weight_concentration_prior_type``::
     |  
     |          (float, float) if 'dirichlet_process' (Beta parameters),
     |          float          if 'dirichlet_distribution' (Dirichlet parameters).
     |  
     |      The higher concentration puts more mass in
     |      the center and will lead to more components being active, while a lower
     |      concentration parameter will lead to more mass at the edge of the
     |      simplex.
     |  
     |  weight_concentration_ : array-like of shape (n_components,)
     |      The dirichlet concentration of each component on the weight
     |      distribution (Dirichlet).
     |  
     |  mean_precision_prior_ : float
     |      The precision prior on the mean distribution (Gaussian).
     |      Controls the extent of where means can be placed.
     |      Larger values concentrate the cluster means around `mean_prior`.
     |      If mean_precision_prior is set to None, `mean_precision_prior_` is set
     |      to 1.
     |  
     |  mean_precision_ : array-like of shape (n_components,)
     |      The precision of each components on the mean distribution (Gaussian).
     |  
     |  mean_prior_ : array-like of shape (n_features,)
     |      The prior on the mean distribution (Gaussian).
     |  
     |  degrees_of_freedom_prior_ : float
     |      The prior of the number of degrees of freedom on the covariance
     |      distributions (Wishart).
     |  
     |  degrees_of_freedom_ : array-like of shape (n_components,)
     |      The number of degrees of freedom of each components in the model.
     |  
     |  covariance_prior_ : float or array-like
     |      The prior on the covariance distribution (Wishart).
     |      The shape depends on `covariance_type`::
     |  
     |          (n_features, n_features) if 'full',
     |          (n_features, n_features) if 'tied',
     |          (n_features)             if 'diag',
     |          float                    if 'spherical'
     |  
     |  Examples
     |  --------
     |  >>> import numpy as np
     |  >>> from sklearn.mixture import BayesianGaussianMixture
     |  >>> X = np.array([[1, 2], [1, 4], [1, 0], [4, 2], [12, 4], [10, 7]])
     |  >>> bgm = BayesianGaussianMixture(n_components=2, random_state=42).fit(X)
     |  >>> bgm.means_
     |  array([[2.49... , 2.29...],
     |         [8.45..., 4.52... ]])
     |  >>> bgm.predict([[0, 0], [9, 3]])
     |  array([0, 1])
     |  
     |  See Also
     |  --------
     |  GaussianMixture : Finite Gaussian mixture fit with EM.
     |  
     |  References
     |  ----------
     |  
     |  .. [1] `Bishop, Christopher M. (2006). "Pattern recognition and machine
     |     learning". Vol. 4 No. 4. New York: Springer.
     |     <https://www.springer.com/kr/book/9780387310732>`_
     |  
     |  .. [2] `Hagai Attias. (2000). "A Variational Bayesian Framework for
     |     Graphical Models". In Advances in Neural Information Processing
     |     Systems 12.
     |     <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.36.2841&rep=rep1&type=pdf>`_
     |  
     |  .. [3] `Blei, David M. and Michael I. Jordan. (2006). "Variational
     |     inference for Dirichlet process mixtures". Bayesian analysis 1.1
     |     <https://www.cs.princeton.edu/courses/archive/fall11/cos597C/reading/BleiJordan2005.pdf>`_
     |  
     |  Method resolution order:
     |      BayesianGaussianMixture
     |      sklearn.mixture._base.BaseMixture
     |      sklearn.base.DensityMixin
     |      sklearn.base.BaseEstimator
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  ____iinniitt____(self, *, n_components=1, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weight_concentration_prior_type='dirichlet_process', weight_concentration_prior=None, mean_precision_prior=None, mean_prior=None, degrees_of_freedom_prior=None, covariance_prior=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |  
     |  ____aabbssttrraaccttmmeetthhooddss____ = frozenset()
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.mixture._base.BaseMixture:
     |  
     |  ffiitt(self, X, y=None)
     |      Estimate model parameters with the EM algorithm.
     |      
     |      The method fits the model ``n_init`` times and sets the parameters with
     |      which the model has the largest likelihood or lower bound. Within each
     |      trial, the method iterates between E-step and M-step for ``max_iter``
     |      times until the change of likelihood or lower bound is less than
     |      ``tol``, otherwise, a ``ConvergenceWarning`` is raised.
     |      If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single
     |      initialization is performed upon the first call. Upon consecutive
     |      calls, training starts where it left off.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      self
     |  
     |  ffiitt__pprreeddiicctt(self, X, y=None)
     |      Estimate model parameters using X and predict the labels for X.
     |      
     |      The method fits the model n_init times and sets the parameters with
     |      which the model has the largest likelihood or lower bound. Within each
     |      trial, the method iterates between E-step and M-step for `max_iter`
     |      times until the change of likelihood or lower bound is less than
     |      `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is
     |      raised. After fitting, it predicts the most probable label for the
     |      input data points.
     |      
     |      .. versionadded:: 0.20
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      labels : array, shape (n_samples,)
     |          Component labels.
     |  
     |  pprreeddiicctt(self, X)
     |      Predict the labels for the data samples in X using trained model.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      labels : array, shape (n_samples,)
     |          Component labels.
     |  
     |  pprreeddiicctt__pprroobbaa(self, X)
     |      Predict posterior probability of each component given the data.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      resp : array, shape (n_samples, n_components)
     |          Returns the probability each Gaussian (state) in
     |          the model given each sample.
     |  
     |  ssaammppllee(self, n_samples=1)
     |      Generate random samples from the fitted Gaussian distribution.
     |      
     |      Parameters
     |      ----------
     |      n_samples : int, default=1
     |          Number of samples to generate.
     |      
     |      Returns
     |      -------
     |      X : array, shape (n_samples, n_features)
     |          Randomly generated sample
     |      
     |      y : array, shape (nsamples,)
     |          Component labels
     |  
     |  ssccoorree(self, X, y=None)
     |      Compute the per-sample average log-likelihood of the given data X.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_dimensions)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      log_likelihood : float
     |          Log likelihood of the Gaussian mixture given X.
     |  
     |  ssccoorree__ssaammpplleess(self, X)
     |      Compute the weighted log probabilities for each sample.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      log_prob : array, shape (n_samples,)
     |          Log probabilities of each data point in X.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.base.DensityMixin:
     |  
     |  ____ddiicctt____
     |      dictionary for instance variables (if defined)
     |  
     |  ____wweeaakkrreeff____
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |  
     |  ____ggeettssttaattee____(self)
     |  
     |  ____rreepprr____(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |  
     |  ____sseettssttaattee____(self, state)
     |  
     |  ggeett__ppaarraammss(self, deep=True)
     |      Get parameters for this estimator.
     |      
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |      
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |  
     |  sseett__ppaarraammss(self, **params)
     |      Set the parameters of this estimator.
     |      
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``<component>__<parameter>`` so that it's
     |      possible to update each component of a nested object.
     |      
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |      
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.
    
    class GGaauussssiiaannMMiixxttuurree(sklearn.mixture._base.BaseMixture)
     |  GaussianMixture(n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)
     |  
     |  Gaussian Mixture.
     |  
     |  Representation of a Gaussian mixture model probability distribution.
     |  This class allows to estimate the parameters of a Gaussian mixture
     |  distribution.
     |  
     |  Read more in the :ref:`User Guide <gmm>`.
     |  
     |  .. versionadded:: 0.18
     |  
     |  Parameters
     |  ----------
     |  n_components : int, default=1
     |      The number of mixture components.
     |  
     |  covariance_type : {'full', 'tied', 'diag', 'spherical'}, default='full'
     |      String describing the type of covariance parameters to use.
     |      Must be one of:
     |  
     |      'full'
     |          each component has its own general covariance matrix
     |      'tied'
     |          all components share the same general covariance matrix
     |      'diag'
     |          each component has its own diagonal covariance matrix
     |      'spherical'
     |          each component has its own single variance
     |  
     |  tol : float, default=1e-3
     |      The convergence threshold. EM iterations will stop when the
     |      lower bound average gain is below this threshold.
     |  
     |  reg_covar : float, default=1e-6
     |      Non-negative regularization added to the diagonal of covariance.
     |      Allows to assure that the covariance matrices are all positive.
     |  
     |  max_iter : int, default=100
     |      The number of EM iterations to perform.
     |  
     |  n_init : int, default=1
     |      The number of initializations to perform. The best results are kept.
     |  
     |  init_params : {'kmeans', 'random'}, default='kmeans'
     |      The method used to initialize the weights, the means and the
     |      precisions.
     |      Must be one of::
     |  
     |          'kmeans' : responsibilities are initialized using kmeans.
     |          'random' : responsibilities are initialized randomly.
     |  
     |  weights_init : array-like of shape (n_components, ), default=None
     |      The user-provided initial weights.
     |      If it is None, weights are initialized using the `init_params` method.
     |  
     |  means_init : array-like of shape (n_components, n_features), default=None
     |      The user-provided initial means,
     |      If it is None, means are initialized using the `init_params` method.
     |  
     |  precisions_init : array-like, default=None
     |      The user-provided initial precisions (inverse of the covariance
     |      matrices).
     |      If it is None, precisions are initialized using the 'init_params'
     |      method.
     |      The shape depends on 'covariance_type'::
     |  
     |          (n_components,)                        if 'spherical',
     |          (n_features, n_features)               if 'tied',
     |          (n_components, n_features)             if 'diag',
     |          (n_components, n_features, n_features) if 'full'
     |  
     |  random_state : int, RandomState instance or None, default=None
     |      Controls the random seed given to the method chosen to initialize the
     |      parameters (see `init_params`).
     |      In addition, it controls the generation of random samples from the
     |      fitted distribution (see the method `sample`).
     |      Pass an int for reproducible output across multiple function calls.
     |      See :term:`Glossary <random_state>`.
     |  
     |  warm_start : bool, default=False
     |      If 'warm_start' is True, the solution of the last fitting is used as
     |      initialization for the next call of fit(). This can speed up
     |      convergence when fit is called several times on similar problems.
     |      In that case, 'n_init' is ignored and only a single initialization
     |      occurs upon the first call.
     |      See :term:`the Glossary <warm_start>`.
     |  
     |  verbose : int, default=0
     |      Enable verbose output. If 1 then it prints the current
     |      initialization and each iteration step. If greater than 1 then
     |      it prints also the log probability and the time needed
     |      for each step.
     |  
     |  verbose_interval : int, default=10
     |      Number of iteration done before the next print.
     |  
     |  Attributes
     |  ----------
     |  weights_ : array-like of shape (n_components,)
     |      The weights of each mixture components.
     |  
     |  means_ : array-like of shape (n_components, n_features)
     |      The mean of each mixture component.
     |  
     |  covariances_ : array-like
     |      The covariance of each mixture component.
     |      The shape depends on `covariance_type`::
     |  
     |          (n_components,)                        if 'spherical',
     |          (n_features, n_features)               if 'tied',
     |          (n_components, n_features)             if 'diag',
     |          (n_components, n_features, n_features) if 'full'
     |  
     |  precisions_ : array-like
     |      The precision matrices for each component in the mixture. A precision
     |      matrix is the inverse of a covariance matrix. A covariance matrix is
     |      symmetric positive definite so the mixture of Gaussian can be
     |      equivalently parameterized by the precision matrices. Storing the
     |      precision matrices instead of the covariance matrices makes it more
     |      efficient to compute the log-likelihood of new samples at test time.
     |      The shape depends on `covariance_type`::
     |  
     |          (n_components,)                        if 'spherical',
     |          (n_features, n_features)               if 'tied',
     |          (n_components, n_features)             if 'diag',
     |          (n_components, n_features, n_features) if 'full'
     |  
     |  precisions_cholesky_ : array-like
     |      The cholesky decomposition of the precision matrices of each mixture
     |      component. A precision matrix is the inverse of a covariance matrix.
     |      A covariance matrix is symmetric positive definite so the mixture of
     |      Gaussian can be equivalently parameterized by the precision matrices.
     |      Storing the precision matrices instead of the covariance matrices makes
     |      it more efficient to compute the log-likelihood of new samples at test
     |      time. The shape depends on `covariance_type`::
     |  
     |          (n_components,)                        if 'spherical',
     |          (n_features, n_features)               if 'tied',
     |          (n_components, n_features)             if 'diag',
     |          (n_components, n_features, n_features) if 'full'
     |  
     |  converged_ : bool
     |      True when convergence was reached in fit(), False otherwise.
     |  
     |  n_iter_ : int
     |      Number of step used by the best fit of EM to reach the convergence.
     |  
     |  lower_bound_ : float
     |      Lower bound value on the log-likelihood (of the training data with
     |      respect to the model) of the best fit of EM.
     |  
     |  Examples
     |  --------
     |  >>> import numpy as np
     |  >>> from sklearn.mixture import GaussianMixture
     |  >>> X = np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])
     |  >>> gm = GaussianMixture(n_components=2, random_state=0).fit(X)
     |  >>> gm.means_
     |  array([[10.,  2.],
     |         [ 1.,  2.]])
     |  >>> gm.predict([[0, 0], [12, 3]])
     |  array([1, 0])
     |  
     |  See Also
     |  --------
     |  BayesianGaussianMixture : Gaussian mixture model fit with a variational
     |      inference.
     |  
     |  Method resolution order:
     |      GaussianMixture
     |      sklearn.mixture._base.BaseMixture
     |      sklearn.base.DensityMixin
     |      sklearn.base.BaseEstimator
     |      builtins.object
     |  
     |  Methods defined here:
     |  
     |  ____iinniitt____(self, n_components=1, *, covariance_type='full', tol=0.001, reg_covar=1e-06, max_iter=100, n_init=1, init_params='kmeans', weights_init=None, means_init=None, precisions_init=None, random_state=None, warm_start=False, verbose=0, verbose_interval=10)
     |      Initialize self.  See help(type(self)) for accurate signature.
     |  
     |  aaiicc(self, X)
     |      Akaike information criterion for the current model on the input X.
     |      
     |      Parameters
     |      ----------
     |      X : array of shape (n_samples, n_dimensions)
     |      
     |      Returns
     |      -------
     |      aic : float
     |          The lower the better.
     |  
     |  bbiicc(self, X)
     |      Bayesian information criterion for the current model on the input X.
     |      
     |      Parameters
     |      ----------
     |      X : array of shape (n_samples, n_dimensions)
     |      
     |      Returns
     |      -------
     |      bic : float
     |          The lower the better.
     |  
     |  ----------------------------------------------------------------------
     |  Data and other attributes defined here:
     |  
     |  ____aabbssttrraaccttmmeetthhooddss____ = frozenset()
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.mixture._base.BaseMixture:
     |  
     |  ffiitt(self, X, y=None)
     |      Estimate model parameters with the EM algorithm.
     |      
     |      The method fits the model ``n_init`` times and sets the parameters with
     |      which the model has the largest likelihood or lower bound. Within each
     |      trial, the method iterates between E-step and M-step for ``max_iter``
     |      times until the change of likelihood or lower bound is less than
     |      ``tol``, otherwise, a ``ConvergenceWarning`` is raised.
     |      If ``warm_start`` is ``True``, then ``n_init`` is ignored and a single
     |      initialization is performed upon the first call. Upon consecutive
     |      calls, training starts where it left off.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      self
     |  
     |  ffiitt__pprreeddiicctt(self, X, y=None)
     |      Estimate model parameters using X and predict the labels for X.
     |      
     |      The method fits the model n_init times and sets the parameters with
     |      which the model has the largest likelihood or lower bound. Within each
     |      trial, the method iterates between E-step and M-step for `max_iter`
     |      times until the change of likelihood or lower bound is less than
     |      `tol`, otherwise, a :class:`~sklearn.exceptions.ConvergenceWarning` is
     |      raised. After fitting, it predicts the most probable label for the
     |      input data points.
     |      
     |      .. versionadded:: 0.20
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      labels : array, shape (n_samples,)
     |          Component labels.
     |  
     |  pprreeddiicctt(self, X)
     |      Predict the labels for the data samples in X using trained model.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      labels : array, shape (n_samples,)
     |          Component labels.
     |  
     |  pprreeddiicctt__pprroobbaa(self, X)
     |      Predict posterior probability of each component given the data.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      resp : array, shape (n_samples, n_components)
     |          Returns the probability each Gaussian (state) in
     |          the model given each sample.
     |  
     |  ssaammppllee(self, n_samples=1)
     |      Generate random samples from the fitted Gaussian distribution.
     |      
     |      Parameters
     |      ----------
     |      n_samples : int, default=1
     |          Number of samples to generate.
     |      
     |      Returns
     |      -------
     |      X : array, shape (n_samples, n_features)
     |          Randomly generated sample
     |      
     |      y : array, shape (nsamples,)
     |          Component labels
     |  
     |  ssccoorree(self, X, y=None)
     |      Compute the per-sample average log-likelihood of the given data X.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_dimensions)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      log_likelihood : float
     |          Log likelihood of the Gaussian mixture given X.
     |  
     |  ssccoorree__ssaammpplleess(self, X)
     |      Compute the weighted log probabilities for each sample.
     |      
     |      Parameters
     |      ----------
     |      X : array-like of shape (n_samples, n_features)
     |          List of n_features-dimensional data points. Each row
     |          corresponds to a single data point.
     |      
     |      Returns
     |      -------
     |      log_prob : array, shape (n_samples,)
     |          Log probabilities of each data point in X.
     |  
     |  ----------------------------------------------------------------------
     |  Data descriptors inherited from sklearn.base.DensityMixin:
     |  
     |  ____ddiicctt____
     |      dictionary for instance variables (if defined)
     |  
     |  ____wweeaakkrreeff____
     |      list of weak references to the object (if defined)
     |  
     |  ----------------------------------------------------------------------
     |  Methods inherited from sklearn.base.BaseEstimator:
     |  
     |  ____ggeettssttaattee____(self)
     |  
     |  ____rreepprr____(self, N_CHAR_MAX=700)
     |      Return repr(self).
     |  
     |  ____sseettssttaattee____(self, state)
     |  
     |  ggeett__ppaarraammss(self, deep=True)
     |      Get parameters for this estimator.
     |      
     |      Parameters
     |      ----------
     |      deep : bool, default=True
     |          If True, will return the parameters for this estimator and
     |          contained subobjects that are estimators.
     |      
     |      Returns
     |      -------
     |      params : dict
     |          Parameter names mapped to their values.
     |  
     |  sseett__ppaarraammss(self, **params)
     |      Set the parameters of this estimator.
     |      
     |      The method works on simple estimators as well as on nested objects
     |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have
     |      parameters of the form ``<component>__<parameter>`` so that it's
     |      possible to update each component of a nested object.
     |      
     |      Parameters
     |      ----------
     |      **params : dict
     |          Estimator parameters.
     |      
     |      Returns
     |      -------
     |      self : estimator instance
     |          Estimator instance.

DDAATTAA
    ____aallll____ = ['GaussianMixture', 'BayesianGaussianMixture']

FFIILLEE
    /home/cv1/hangchen2/anaconda3/lib/python3.8/site-packages/sklearn/mixture/__init__.py

